{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "This notebook presents the whole data cleaning process, which consists in extracting new tables and relations, as well as cleaning the existing files from dirty tuples and values.\n",
    "\n",
    "The new clean data files are saved in the `.csv` format, and will be used to load data to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "We first import all the `.csv` files into `pandas` DataFrames.\n",
    "\n",
    "_Note_: some lines are ill-formed, we choose to ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (2,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Root of the data files\n",
    "PATH = os.path.join('..', 'data', 'original')\n",
    "\n",
    "# Dic: name -> dataframe\n",
    "dataframes = {}\n",
    "\n",
    "# Get all the original files\n",
    "for file in os.listdir(PATH):\n",
    "    # Skip hidden files\n",
    "    if (file.startswith('.')):\n",
    "        continue\n",
    "        \n",
    "    name = file.split('.')[0]\n",
    "    # Note: some lines are ill-formed, we ignore them\n",
    "    dataframes[name] = pd.read_csv(os.path.join(PATH, file), encoding='utf-8', quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes table\n",
    "\n",
    "This part aims to extract the notes from each table containing a `notes` attributes. Notes are loaded in a new table, and replaced by foreign keys in the original tables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first concatenate all the notes from all the dataframes. And create the new note dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Entire book available from gutenberg.org at ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Auction at cqout.com in August 2007 states \"De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Theatrical giveaway depicting a small portion ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Title from title page.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Best-known Wilhelm Busch book; The Katzenjamme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              notes\n",
       "1   1  Entire book available from gutenberg.org at ht...\n",
       "2   2  Auction at cqout.com in August 2007 states \"De...\n",
       "3   3  Theatrical giveaway depicting a small portion ...\n",
       "4   4                             Title from title page.\n",
       "5   5  Best-known Wilhelm Busch book; The Katzenjamme..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes = pd.Series()\n",
    "\n",
    "# Get all the notes from all the dataframes containing notes\n",
    "for _, df in dataframes.items():\n",
    "    if 'notes' in df.columns: \n",
    "        notes = notes.append(df['notes'].dropna(), ignore_index=True)\n",
    "    \n",
    "    if 'reprint_notes' in df.columns:\n",
    "        notes = notes.append(df['reprint_notes'].dropna(), ignore_index=True)\n",
    "\n",
    "notes_df = utils.extract_table(notes, 'notes')\n",
    "dataframes['notes'] = notes_df\n",
    "notes_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the notes by the IDs in the original tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name, df in dataframes.items():\n",
    "    # Skip the notes dataframe obviously\n",
    "    if name == 'notes':\n",
    "        continue\n",
    "    \n",
    "    if 'notes' in df.columns:\n",
    "        # Map notes to their IDs\n",
    "        df['notes_id'] = utils.map_column(df['notes'], dataframes['notes'], 'id', 'notes')\n",
    "        df.drop('notes', axis=1, inplace=True)\n",
    "        \n",
    "    if 'reprint_notes' in df.columns:\n",
    "        df['reprint_notes_id'] = utils.map_column(df['reprint_notes'], dataframes['notes'], 'id', 'notes')\n",
    "        df.drop('reprint_notes', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        NaN\n",
       "1    86501.0\n",
       "2    86502.0\n",
       "3    86502.0\n",
       "4        NaN\n",
       "Name: notes_id, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['story']['notes_id'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         NaN\n",
       "1         NaN\n",
       "2         NaN\n",
       "3         NaN\n",
       "4         NaN\n",
       "5    377020.0\n",
       "6         NaN\n",
       "7    377021.0\n",
       "8    377021.0\n",
       "9         NaN\n",
       "Name: reprint_notes_id, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['story']['reprint_notes_id'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First/Last issue Relation\n",
    "\n",
    "We noticed that there is a cyclic dependency between the tables _Issues_ and _Series_, since issues belong to a serie, and series have a first and last issue. It's generally a bad idea (and impossible practically) to create such cyclic relations between tables. So we decide to create a new relation *First_last_issue* to link series with their first and last issue, and remove the reference to _Issues_ in _Series_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serie_id</th>\n",
       "      <th>first_issue_id</th>\n",
       "      <th>last_issue_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   serie_id  first_issue_id  last_issue_id\n",
       "0         1             1.0            1.0\n",
       "1         2             2.0            2.0\n",
       "2         3             3.0            3.0\n",
       "3         4             6.0            6.0\n",
       "4         5             4.0            4.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract relation\n",
    "first_last_issue = dataframes['series'][['id', 'first_issue_id', 'last_issue_id']]\n",
    "\n",
    "# Rename the columns\n",
    "first_last_issue.columns = ['serie_id', 'first_issue_id', 'last_issue_id']\n",
    "\n",
    "# Remove rows if first_issue_id and last_issue_id are both NULL\n",
    "first_last_issue = first_last_issue.dropna(subset=['first_issue_id', 'last_issue_id'], how='all')\n",
    "\n",
    "# Save the new relation\n",
    "dataframes['first_last_issue'] = first_last_issue\n",
    "\n",
    "first_last_issue.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now drop the *first_issue_id* and *last_issue_id* columns of _Series_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframes['series'] = dataframes['series'].drop(['first_issue_id', 'last_issue_id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artists table\n",
    "\n",
    "We first scan through all the different categories of artists , clean the data and then store all artist in one single table as described in our ER diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make table to store the list of all artists\n",
    "all_artists = pd.Series()\n",
    "# Dictionnary to store all artists of one category\n",
    "artists = {}\n",
    "categories = ['script', 'pencils', 'inks', 'colors', 'letters']\n",
    "\n",
    "for category in categories:\n",
    "    # Unpack the artists lists so we have all artists for every story\n",
    "    unpacked = utils.unpack_column(dataframes['story'], 'id', category)\n",
    "    \n",
    "    # Clean the unpacked elements \n",
    "    unpacked[category] = utils.clean_column(unpacked[category])\n",
    "    \n",
    "    # We have now our relation with story IDs and artists names\n",
    "    artists[category] = unpacked.dropna(how='any')\n",
    "    \n",
    "    # Add artists to the global artist list\n",
    "    all_artists = all_artists.append(artists[category][category], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now extract our new artists table from the whole list of artists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Gustave Doré</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Harry Rogers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Wilhelm Busch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The Donaldson Brothers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Richard Doyle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                    name\n",
       "1   1            Gustave Doré\n",
       "2   2            Harry Rogers\n",
       "3   3           Wilhelm Busch\n",
       "4   4  The Donaldson Brothers\n",
       "5   5           Richard Doyle"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['artists'] = utils.extract_table(all_artists, 'name')\n",
    "dataframes['artists'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each relation fôf artist, we map the names to the IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for category in categories:\n",
    "    artists[category].columns = ['story_id', 'artist_id']\n",
    "    artists[category]['artist_id'] = utils.map_column(artists[category]['artist_id'], dataframes['artists'], 'id', 'name')\n",
    "    dataframes[category] = artists[category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story_id</th>\n",
       "      <th>artist_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    story_id  artist_id\n",
       "7         13          1\n",
       "8         14          1\n",
       "9         15          1\n",
       "10        16          1\n",
       "11        17          1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['script'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now drop the different artists columns from the original Story dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframes['story'] = dataframes['story'].drop(categories, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual files cleaning\n",
    "\n",
    "This part aims to clean each `.csv` file individually in order to remove dirty rows and clear values that need some special treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Country\n",
    "\n",
    "By browsing the country data, we see that one row is not valid, with ID 248. We see in the cell below that for `publisher`, for example, no row references this ID, which is with high probably pure dirty data, we can safely remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of publisher with country_id 248: 0.\n",
      "NaN values: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id      0\n",
       "code    0\n",
       "name    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub = dataframes['publisher']\n",
    "print('Number of publisher with country_id 248: {}.'.format(len(pub[pub['country_id'] == 248])))\n",
    "\n",
    "# Look for NaN values\n",
    "print('NaN values: ')\n",
    "df = dataframes['country']\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove the desired row\n",
    "dataframes['country'] = df[df['id'] != 248]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Story Reprint\n",
    "\n",
    "The story reprint table needs to be full, as we don't accept _NULL_ foreign keys in this case. We see in the cell below that there are no empty cells in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "origin_id    0\n",
       "target_id    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['story_reprint'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Story Type\n",
    "\n",
    "By looking at the story types we see that the third row is problematic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                             3\n",
       "name    (backcovers) *do not use* / *please fix*\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataframes['story_type']\n",
    "df.ix[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check if any story contains a reference to this row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stories referencing ID 3: 0.\n"
     ]
    }
   ],
   "source": [
    "stories = dataframes['story']\n",
    "print('Number of stories referencing ID 3: {}.'.format(len(stories[stories['type_id'] == 3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can safely remove it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframes['story_type'] = df[df['id'] != 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language\n",
    "\n",
    "Looking at the language file, all the rows are clean and it's safe to keep them as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id      0\n",
       "code    0\n",
       "name    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['language'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brang group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 0\n",
       "name               0\n",
       "year_began      2938\n",
       "year_ended      3857\n",
       "notes_id        4615\n",
       "url             4701\n",
       "publisher_id       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['brand_group'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the essential attributes don't have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Marvel                  17\n",
       "DC                      10\n",
       "Dargaud                  8\n",
       "A                        7\n",
       "Classics Illustrated     6\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['brand_group']['name'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we see that there are quite a lot of duplicates in the names. But if we look at the cell below, for the same names, we have each time different *publisher_id*s, so it makes sense to keep these duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2105,  613, 3434,   78, 4720, 3174, 4437,  592, 3029, 8492, 7151,\n",
       "       2195, 5905, 1798, 1977, 3655, 6917])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['brand_group'][dataframes['brand_group']['name'] == 'Marvel']['publisher_id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series Publication types\n",
    "Obviously this table is ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>magazine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      name\n",
       "0   1      book\n",
       "1   2  magazine\n",
       "2   3     album"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['series_publication_type'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue Reprint\n",
    "\n",
    "We make sure there is no null rows in the reprint table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 0\n",
       "origin_issue_id    0\n",
       "target_issue_id    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['issue_reprint'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indicia Publisher\n",
    "\n",
    "For this table we need to make sure the *publisher_id* attribute is not null, which is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 0\n",
       "name               0\n",
       "publisher_id       0\n",
       "country_id         0\n",
       "year_began      2612\n",
       "year_ended      3563\n",
       "is_surrogate       0\n",
       "notes_id        4282\n",
       "url             4711\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['indicia_publisher'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving files\n",
    "\n",
    "We can now save our clean and new tables, ready for database loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = os.path.join('..', 'data', 'clean')\n",
    "\n",
    "#for name, df in dataframes.items():\n",
    "#    df.to_csv(name.title() + '.csv', index=False, float_format='%.0f')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we collect the max and average length of string attributes for each column of each table, in order to help use choosing right string lengths for the databas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lengths = {}\n",
    "for name, df in dataframes.items():\n",
    "    tmp = {}\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if (col_type == np.dtype('O') and type(df[col].dropna().iloc[0]) == str) or col_type == np.dtype(str):\n",
    "            strs = df[col].dropna().str.len()\n",
    "            tmp[col] = {'min': int(min(strs)),\n",
    "                        'max': int(max(strs)),\n",
    "                        'ave': int(sum(strs) / len(strs))}\n",
    "    if len(tmp) > 0:        \n",
    "        lengths[name] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(OUTPUT_PATH, 'lengths.json'), 'w') as file:\n",
    "    json.dump([lengths], file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
