{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "This notebook presents the whole data cleaning process, which consists in extracting new tables and relations, as well as cleaning the existing files from dirty tuples and values.\n",
    "\n",
    "The new clean data files are saved in the `.csv` format, and will be used to load data to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "We first import all the `.csv` files into `pandas` DataFrames.\n",
    "\n",
    "_Note_: some lines are ill-formed, we choose to ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (2,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Root of the data files\n",
    "PATH = os.path.join('..', 'data', 'original')\n",
    "\n",
    "# Dic: name -> dataframe\n",
    "dataframes = {}\n",
    "\n",
    "# Get all the original files\n",
    "for file in os.listdir(PATH):\n",
    "    # Skip hidden files\n",
    "    if (file.startswith('.')):\n",
    "        continue\n",
    "        \n",
    "    name = file.split('.')[0]\n",
    "    # Note: some lines are ill-formed, we ignore them\n",
    "    dataframes[name] = pd.read_csv(os.path.join(PATH, file), encoding='utf-8', quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes table\n",
    "\n",
    "This part aims to extract the notes from each table containing a `notes` attributes. Notes are loaded in a new table, and replaced by foreign keys in the original tables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first concatenate all the notes from all the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    Lettering credit from Dick Ayers via Mike Quil...\n",
       "2    Lettering credit from Dick Ayers via Mike Quil...\n",
       "3                                Single panel cartoons\n",
       "4    Victorian style comic (no word balloons; brief...\n",
       "5                                single panel cartoons\n",
       "dtype: object"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes = pd.Series()\n",
    "for _, df in dataframes.items():\n",
    "    if 'notes' in df.columns: \n",
    "        note = df['notes'].dropna()\n",
    "        notes = notes.append(note, ignore_index=True)\n",
    "\n",
    "# Keep unique notes\n",
    "notes = notes.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "# Shift to start with ID 1\n",
    "notes.index = notes.index + 1\n",
    "notes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the new notes dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>notes</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lettering credit from Dick Ayers via Mike Quil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lettering credit from Dick Ayers via Mike Quil...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Single panel cartoons</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Victorian style comic (no word balloons; brief...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>single panel cartoons</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               notes  id\n",
       "1  Lettering credit from Dick Ayers via Mike Quil...   1\n",
       "2  Lettering credit from Dick Ayers via Mike Quil...   2\n",
       "3                              Single panel cartoons   3\n",
       "4  Victorian style comic (no word balloons; brief...   4\n",
       "5                              single panel cartoons   5"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Form a DataFrame from the note Series\n",
    "notes_df = notes.to_frame()\n",
    "notes_df.columns = ['notes']\n",
    "notes_df['id'] = notes_df.index\n",
    "\n",
    "dataframes['notes'] = notes_df[['id', 'notes']]\n",
    "notes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the notes by the IDs in the original tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Series from notes to uniqueID\n",
    "notes_mapper = pd.Series(notes.index, index=notes)\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    # Skip the notes dataframe obviously\n",
    "    if name == 'notes':\n",
    "        continue\n",
    "    \n",
    "    if 'notes' in df.columns:\n",
    "        # Map notes to their IDs\n",
    "        df['notes'] = df['notes'].map(notes_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First/Last issue Relation\n",
    "\n",
    "We noticed that there is a cyclic dependency between the tables _Issues_ and _Series_, since issues belong to a serie, and series have a first and last issue. It's generally a bad idea (and impossible practically) to create such cyclic relations between tables. So we decide to create a new relation *First_last_issue* to link series with their first and last issue, and remove the reference to _Issues_ in _Series_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serie_id</th>\n",
       "      <th>first_issue_id</th>\n",
       "      <th>last_issue_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   serie_id  first_issue_id  last_issue_id\n",
       "0         1             1.0            1.0\n",
       "1         2             2.0            2.0\n",
       "2         3             3.0            3.0\n",
       "3         4             6.0            6.0\n",
       "4         5             4.0            4.0"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract relation\n",
    "first_last_issue = dataframes['series'][['id', 'first_issue_id', 'last_issue_id']]\n",
    "\n",
    "# Rename the columns\n",
    "first_last_issue.columns = ['serie_id', 'first_issue_id', 'last_issue_id']\n",
    "\n",
    "# Remove rows if first_issue_id and last_issue_id are both NULL\n",
    "first_last_issue = first_last_issue.dropna(subset=['first_issue_id', 'last_issue_id'], how='all')\n",
    "\n",
    "# Save the new relation\n",
    "dataframes['first_last_issue'] = first_last_issue\n",
    "\n",
    "first_last_issue.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now drop the *first_issue_id* and *last_issue_id* coumns of _Series_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframes['series'] = dataframes['series'].drop(['first_issue_id', 'last_issue_id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artists table\n",
    "\n",
    "We first scan through all the different categories of artists , clean the data and then store all artist in one single table as described in our ER diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                   Gustave Doré\n",
       "2    Wilhelm Busch; Harry Rogers\n",
       "3         The Donaldson Brothers\n",
       "4                  Wilhelm Busch\n",
       "5                  Richard Doyle\n",
       "dtype: object"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make table to store the list of all artists\n",
    "all_artists = pd.Series()\n",
    "# Dictionnary to store all artists of one category\n",
    "artists = {}\n",
    "categories = ['script','pencils','inks','colors','letters']\n",
    "story_df = dataframes['story']\n",
    "\n",
    "for category in categories:\n",
    "    #remove all data in () and in [], removes trailing white space and set empty cells to nan\n",
    "    story_df[category] = story_df[category].str.replace(r\"(\\(.*\\))|(\\[.*\\])|\\?\",\"\").str.strip('; ').replace('',np.nan)\n",
    "\n",
    "    artists[category] = story_df[category]\n",
    "    # Set the index of artists to the corresponding stoy_id\n",
    "    artists[category].index = story_df['id']\n",
    "    #remove nan\n",
    "    artists[category] = artists[category].dropna()\n",
    "    #add artits to the global table\n",
    "    all_artists = all_artists.append(artists[category],ignore_index=True)\n",
    "    \n",
    "# Keep unique artists\n",
    "all_artists = all_artists.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "# Shift to start with ID 1\n",
    "all_artists.index = all_artists.index + 1\n",
    "all_artists.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we correctly index the different category of artists table with the correct story index and the correct artists index. We also save these artists in the dataframe where we store all our tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gustave Doré</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wilhelm Busch; Harry Rogers</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Donaldson Brothers</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wilhelm Busch</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Richard Doyle</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name  id\n",
       "1                 Gustave Doré   1\n",
       "2  Wilhelm Busch; Harry Rogers   2\n",
       "3       The Donaldson Brothers   3\n",
       "4                Wilhelm Busch   4\n",
       "5                Richard Doyle   5"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Series from artists to uniqueID\n",
    "artist_mapper = pd.Series(all_artists.index, index=all_artists)\n",
    "\n",
    "for category in categories:\n",
    "    artists[category] = artists[category].to_frame() \n",
    "    artists[category]['story_id'] = artists[category].index\n",
    "    artists[category].columns = ['artist_id','story_id']\n",
    "    artists[category]['artist_id'] = artists[category]['artist_id'].map(artist_mapper)\n",
    "    \n",
    "    dataframes[category] = artists[category]\n",
    "    \n",
    "\n",
    "df = all_artists.to_frame()\n",
    "df.columns = ['name']\n",
    "df['id'] = df.index\n",
    "dataframes['artists'] = df\n",
    "dataframes['artists'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    story_df = story_df.drop(category,axis = 1)\n",
    "dataframes['story']= story_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual files cleaning\n",
    "\n",
    "This part aims to clean each `.csv` file individually in order to remove dirty rows and clear values that need some special treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Country\n",
    "\n",
    "By browsing the country data, we see that one row is not valid, with ID 248. We see in the cell below that for `publisher`, for example, no row references this ID, which is with high probably pure dirty data, we can safely remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of publisher with country_id 248: 0.\n",
      "NaN values: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id      0\n",
       "code    0\n",
       "name    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub = dataframes['publisher']\n",
    "print('Number of publisher with country_id 248: {}.'.format(len(pub[pub['country_id'] == 248])))\n",
    "\n",
    "# Look for NaN values\n",
    "print('NaN values: ')\n",
    "df = dataframes['country']\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove the desired row\n",
    "dataframes['country'] = df[df['id'] != 248]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Story Reprint\n",
    "\n",
    "The story reprint table needs to be full, as we don't accept _NULL_ foreign keys in this case. We see in the cell below that there are no empty cells in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "origin_id    0\n",
       "target_id    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['story_reprint'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Story Type\n",
    "\n",
    "By looking at the story types we see that the third row is problematic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                             3\n",
       "name    (backcovers) *do not use* / *please fix*\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataframes['story_type']\n",
    "df.ix[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check if any story contains a reference to this row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stories referencing ID 3: 0.\n"
     ]
    }
   ],
   "source": [
    "stories = dataframes['story']\n",
    "print('Number of stories referencing ID 3: {}.'.format(len(stories[stories['type_id'] == 3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can safely remove it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframes['story_type'] = df[df['id'] != 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language\n",
    "\n",
    "Looking at the language file, all the rows are clean and it's safe to keep them as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id      0\n",
       "code    0\n",
       "name    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['language'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brang group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 0\n",
       "name               0\n",
       "year_began      2938\n",
       "year_ended      3857\n",
       "notes           4615\n",
       "url             4701\n",
       "publisher_id       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['brand_group'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the essential attributes don't have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Marvel                  17\n",
       "DC                      10\n",
       "Dargaud                  8\n",
       "A                        7\n",
       "Classics Illustrated     6\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['brand_group']['name'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we see that there are quite a lot of duplicates in the names. But if we look at the cell below, for the same names, we have each time different *publisher_id*s, so it makes sense to keep these duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2105,  613, 3434,   78, 4720, 3174, 4437,  592, 3029, 8492, 7151,\n",
       "       2195, 5905, 1798, 1977, 3655, 6917])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['brand_group'][dataframes['brand_group']['name'] == 'Marvel']['publisher_id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series Publication types\n",
    "Obviously this table is ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>magazine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      name\n",
       "0   1      book\n",
       "1   2  magazine\n",
       "2   3     album"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['series_publication_type'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue Reprint\n",
    "\n",
    "We make sure there is no null rows in the reprint table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 0\n",
       "origin_issue_id    0\n",
       "target_issue_id    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['issue_reprint'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indicia Publisher\n",
    "\n",
    "For this table we need to make sure the *publisher_id* attribute is not null, which is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 0\n",
       "name               0\n",
       "publisher_id       0\n",
       "country_id         0\n",
       "year_began      2612\n",
       "year_ended      3563\n",
       "is_surrogate       0\n",
       "notes           4282\n",
       "url             4711\n",
       "dtype: int64"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['indicia_publisher'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving files\n",
    "\n",
    "We can now save our clean and new tables, ready for database loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor name, df in dataframes.items():\\n    df.to_csv(name.title() + '.csv', index=False, float_format='%.0f')\\n\""
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_PATH = os.path.join('..', 'data', 'clean')\n",
    "\n",
    "#for name, df in dataframes.items():\n",
    "#    df.to_csv(name.title() + '.csv', index=False, float_format='%.0f')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we collect the max and average length of string attributes for each column of each table, in order to help use choosing right string lengths for the databas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = {}\n",
    "for name, df in dataframes.items():\n",
    "    tmp = {}\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if (col_type == np.dtype('O') and type(df[col].dropna().iloc[0]) == str) or col_type == np.dtype(str):\n",
    "            strs = df[col].dropna().str.len()\n",
    "            tmp[col] = {'min': int(min(strs)),\n",
    "                        'max': int(max(strs)),\n",
    "                        'ave': int(sum(strs) / len(strs))}\n",
    "    if len(tmp) > 0:        \n",
    "        lengths[name] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(OUTPUT_PATH, 'lengths.json'), 'w') as file:\n",
    "    json.dump([lengths], file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
